{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/ranadeep/miniforge3/envs/http_testing/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.7)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Installing collected packages: multidict, frozenlist, attrs, yarl, aiosignal, aiohttp\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 attrs-23.2.0 frozenlist-1.4.1 multidict-6.0.5 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "! pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import requests\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "class HTTPBenchmark:\n",
    "    def __init__(self, target_url:str, qps: int=1) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the HTTPBenchmark object with the target URL and QPS\n",
    "        Inputs:\n",
    "        - target_url: The URL to test\n",
    "        - qps: The number of requests per second to send\n",
    "        \"\"\"\n",
    "        self.target_url = target_url\n",
    "        self.qps = qps \n",
    "        self.results = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'latencies': []\n",
    "        }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the test\n",
    "        \"\"\"\n",
    "        return f\"HTTP Benchmark: {self.target_url} @ {self.qps} qps\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the test\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def pretty_print_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Pretty print the results of the test\n",
    "        Print in tabular format:\n",
    "        -------\n",
    "        | URL: {self.target_url} |\n",
    "        -------\n",
    "        | Num Requests: {self.results['total_requests']} |\n",
    "        -------\n",
    "        | Num Successful Requests: {self.results['successful_requests']} |\n",
    "        -------\n",
    "        | Num Failed Requests: {self.results['failed_requests']} |\n",
    "        -------\n",
    "        | Average Latency (ms): {self.results['latencies']} |\n",
    "        -------\n",
    "        \"\"\"\n",
    "        \n",
    "        ret_parts = [\n",
    "            f\"URL: {self.target_url}\",\n",
    "            f\"Num Requests: {self.results['total_requests']}\",\n",
    "            f\"Num Successful Requests: {self.results['successful_requests']}\",\n",
    "            f\"Num Failed Requests: {self.results['failed_requests']}\",\n",
    "            f\"Average Latency (ms): {round(sum(self.results['latencies'])/len(self.results['latencies'])*1000,2)}\"\n",
    "        ]\n",
    "\n",
    "        # Use max_len of parts to format the output table's length\n",
    "        max_len = max(len(part) for part in ret_parts)\n",
    "\n",
    "        # Create table\n",
    "        line_sep = '\\n' + '-' * (max_len+4) + '\\n'\n",
    "        table_lines = [ '| ' + part + ' '*(max_len-len(part)+1) + '|' for part in ret_parts]\n",
    "\n",
    "        table = line_sep + line_sep.join(table_lines) + line_sep\n",
    "\n",
    "        print(table)\n",
    "\n",
    "\n",
    "    def send_request(self) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to send a request to the target URL\n",
    "        Process the response and updates the results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = requests.get(self.target_url)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.results['latencies'].append(elapsed_time)\n",
    "            self.results['total_requests'] += 1\n",
    "            if response.status_code == 200:\n",
    "                self.results['successful_requests'] += 1\n",
    "            else:\n",
    "                self.results['failed_requests'] += 1\n",
    "        except requests.RequestException:\n",
    "            self.results['failed_requests'] += 1\n",
    "\n",
    "    def start_test(self, duration: int=10) -> None:\n",
    "        \"\"\"\n",
    "        Start the test for the specified duration\n",
    "        Inputs:\n",
    "        - duration: The duration of the test in seconds\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time <= duration:\n",
    "            thread = Thread(target=self.send_request)\n",
    "            thread.start()\n",
    "            time.sleep(1 / self.qps)\n",
    "        thread.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "| URL: https://fireworks.ai/    |\n",
      "---------------------------------\n",
      "| Num Requests: 17              |\n",
      "---------------------------------\n",
      "| Num Successful Requests: 17   |\n",
      "---------------------------------\n",
      "| Num Failed Requests: 0        |\n",
      "---------------------------------\n",
      "| Average Latency (ms): 4767.64 |\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = HTTPBenchmark('https://fireworks.ai/', 10)\n",
    "benchmark.start_test(duration=2)\n",
    "benchmark.pretty_print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTP Benchmark: https://fireworks.ai/ @ 10 qps"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import Counter\n",
    "\n",
    "class HTTPBenchmark2:\n",
    "    def __init__(self, target_url:str, qps: int=1) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the HTTPBenchmark object with the target URL and QPS\n",
    "        Inputs:\n",
    "        - target_url: The URL to test\n",
    "        - qps: The number of requests per second to send\n",
    "        \"\"\"\n",
    "        self.target_url = target_url\n",
    "        self.qps = qps \n",
    "        self.reset_results()\n",
    "\n",
    "    def reset_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the results of the test\n",
    "        \"\"\"\n",
    "        self.results =  {\n",
    "                'total_requests': 0,\n",
    "                'successful_requests': 0,\n",
    "                'failed_requests': 0,\n",
    "                'latencies': [],\n",
    "                'error_code_count': {}, # Error code and their counts, if any\n",
    "            }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the test\n",
    "        \"\"\"\n",
    "        return f\"HTTP Benchmark: {self.target_url} @ {self.qps} qps\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the test\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def pretty_print_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Pretty print the results of the test\n",
    "        Print in tabular format:\n",
    "        -------\n",
    "        | URL: {self.target_url} |\n",
    "        -------\n",
    "        | Num Requests: {self.results['total_requests']} |\n",
    "        -------\n",
    "        | Num Successful Requests: {self.results['successful_requests']} |\n",
    "        -------\n",
    "        | Num Failed Requests: {self.results['failed_requests']} |\n",
    "        -------\n",
    "        | Average Latency (ms): {self.results['latencies']} |\n",
    "        -------\n",
    "        \"\"\"\n",
    "        \n",
    "        ret_parts = [\n",
    "            f\"URL: {self.target_url}\",\n",
    "            f\"Num Requests: {self.results['total_requests']}\",\n",
    "            f\"Num Successful Requests: {self.results['successful_requests']}\",\n",
    "            f\"Num Failed Requests: {self.results['failed_requests']}\",\n",
    "            f\"Average Latency (ms): {round(sum(self.results['latencies'])/len(self.results['latencies'])*1000,2)}\",\n",
    "            f\"Top {min(3, len(self.results['error_code_count']))} Error Codes: {{code: count for code, count in sorted(self.results['error_code_count'].items(), key=lambda x: x[1], reverse=True)[:3]}}\"\n",
    "        ]\n",
    "\n",
    "        # Use max_len of parts to format the output table's length\n",
    "        max_len = max(len(part) for part in ret_parts)\n",
    "\n",
    "        # Create table\n",
    "        line_sep = '\\n' + '-' * (max_len+4) + '\\n'\n",
    "        table_lines = [ '| ' + part + ' '*(max_len-len(part)+1) + '|' for part in ret_parts]\n",
    "\n",
    "        table = line_sep + line_sep.join(table_lines) + line_sep\n",
    "\n",
    "        print(table)\n",
    "\n",
    "    async def send_request(self, session, url):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            async with session.get(url, timeout=10) as response:\n",
    "                await response.read()  # Ensure the whole response is fetched\n",
    "                return time.time() - start_time, response.status\n",
    "        except Exception as e:\n",
    "            return time.time() - start_time, str(e)  # Capture and return exception message\n",
    "\n",
    "    async def load_test(self, duration):\n",
    "        tasks = []\n",
    "        total_requests = self.qps * duration\n",
    "        interval = 1 / self.qps  # Interval to maintain requests per second\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            start_time = time.time()\n",
    "            for i in range(total_requests):\n",
    "                if time.time() - start_time < duration:\n",
    "                    task = asyncio.create_task(self.send_request(session, self.target_url))\n",
    "                    tasks.append(task)\n",
    "                    await asyncio.sleep(interval)  # Wait for the next request slot\n",
    "                else:\n",
    "                    break  # Stop if the duration is exceeded\n",
    "\n",
    "            results = await asyncio.gather(*tasks)  # Wait for all tasks to complete\n",
    "\n",
    "        self.results['total_requests'] = len(results)\n",
    "        self.results['successful_requests'] = sum(1 for _, status in results if status == 200)\n",
    "        self.results['failed_requests'] = len(results) - self.results['successful_requests']\n",
    "        self.results['latencies'] = [latency for latency, status in results if status == 200]\n",
    "        self.results['error_code_count'] = Counter([status for _, status in results if status != 200])\n",
    "\n",
    "        success_count = sum(1 for _, status in results if isinstance(status, int) and status == 200)\n",
    "        error_count = len(results) - success_count\n",
    "        latencies = [latency for latency, status in results if status == 200]\n",
    "        errors = [error for _, error in results if isinstance(error, str)]\n",
    "\n",
    "        print(f\"Total Requests: {len(results)}\")\n",
    "        print(f\"Successful Requests: {success_count}\")\n",
    "        print(f\"Failed Requests: {error_count}\")\n",
    "        print(f\"Average Latency: {sum(latencies) / len(latencies) if latencies else None:.3f} seconds\")\n",
    "        print(f\"Sample Errors: {errors[:10]}\")  # Print first 10 error messages\n",
    "    \n",
    "\n",
    "    async def start_test(self, duration: int=10) -> None:\n",
    "        \"\"\"\n",
    "        Start the test for the specified duration\n",
    "        Inputs:\n",
    "        - duration: The duration of the test in seconds\n",
    "        \"\"\"\n",
    "        self.reset_results()\n",
    "        asyncio.run(self.load_test(duration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m HTTPBenchmark2(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fireworks.ai/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m benchmark\u001b[38;5;241m.\u001b[39mpretty_print_results()\n",
      "Cell \u001b[0;32mIn[27], line 131\u001b[0m, in \u001b[0;36mHTTPBenchmark2.start_test\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mStart the test for the specified duration\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m- duration: The duration of the test in seconds\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_results()\n\u001b[0;32m--> 131\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/http_testing/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "benchmark = HTTPBenchmark2('https://fireworks.ai/', 10)\n",
    "benchmark.start_test(duration=2)\n",
    "benchmark.pretty_print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --url URL --qps QPS\n",
      "ipykernel_launcher.py: error: the following arguments are required: --url, --qps\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranadeep/miniforge3/envs/http_testing/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"HTTP Benchmark Tool\")\n",
    "parser.add_argument(\"--url\", required=True, help=\"URL to benchmark\")\n",
    "parser.add_argument(\"--qps\", type=int, required=True, help=\"Queries per second\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "benchmark = HTTPBenchmark(args.url, args.qps)\n",
    "benchmark.start_test(duration=60)  # Run for 60 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "http_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
